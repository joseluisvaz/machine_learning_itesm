{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Homework 4 - Jose Vazquez-Espinoza\n",
    "\n",
    "Algorithm's implementation in: https://github.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.stats import multivariate_normal \n",
    "%matplotlib inline\n",
    "\n",
    "#Implementation of Own Algorithm\n",
    "from ML_Algorithms import metrics\n",
    "from ML_Algorithms import utils\n",
    "from ML_Algorithms import SupportVectorMachine as svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (BISHOP):\n",
    "\n",
    "5.5 ) Show that maximazing likelihood for a multiclass neural network model in which the network outputs have the interpretation $y_k(\\mathbf{x}, \\mathbf{w}) = p(t_k = 1 \\mid x)$ is equivalent to the minimization of the cross-entropy error function \n",
    "$$ E(\\mathbf{w})= -\\sum_{n=1}^N \\sum_{k=1}^K t_{kn} \\ln y_k (\\mathbf{x}_n, \\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "For $y_k (x,w)$ we have the conditional distribution (Multinomial) of the labels given the sample $x$ and the parameters $w$ of the neurons. \n",
    "\n",
    "$$p(t | w_1, ... , w_K, x) = \\prod_{k=1}^K y_k(x, w)^{t_k}$$\n",
    "\n",
    "For all the labels and data we have the following likelihood function\n",
    "\n",
    "$$p(T | w_1, ... , w_K , x_1, ... , x_N) = \\prod_{n=1}^N \\prod_{k=1}^K y_{nk}(x_n, w)^{t_nk}$$\n",
    "\n",
    "Maximazing the likelihood is equivalent to minimizing the log likelihood, taking the logarithm of the above expression we have \n",
    "$$\n",
    "-\\ln (p(T | ...)) = \n",
    "-\\ln \\big( \\prod_{n=1}^N \\prod_{k=1}^K y_{nk}(x_n, w_k)^{t_nk} \\big)\n",
    "= - \\sum_{n=1}^N \\ln \\big( \\prod_{k=1}^K y_{nk}(x_n, w_k)^{t_nk} \\big)\n",
    "= - \\sum_{n=1}^N \\sum_{k=1}^K \\ln \\big( y_{nk}(x_n, w_k)^{t_nk} \\big)\n",
    "= - \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\ln \\big( y_{nk}(x_n, w_k) \\big)\n",
    "$$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5.10 ) Consider a Hessian matrix $\\mathbf{H}$ with eigenvector equation\n",
    "\n",
    "$$ \\mathbf{H u}_i = \\lambda_i \\mathbf{u}_i$$\n",
    "\n",
    "By setting the vector $\\mathbf{v}$ in\n",
    "\n",
    "$$\\mathbf{v}^T \\mathbf{Hv} = \\sum_i c_i^2 \\lambda_i$$\n",
    "\n",
    "equal to each of the eigenvectors $\\mathbf{u}_i$ in turn, show that $\\mathbf{H}$ is positive definite if, and only if, all of its eigenvalues are positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "Assuming $H$ is possitive definite and ussing the first equation we have that\n",
    "\n",
    "$$\\lambda_i = \\mathbf{u_i^T H u_i}  > 0 $$\n",
    "\n",
    "using the fact that\n",
    "\n",
    "$$\\mathbf{v} = \\sum_i c_i \\mathbf{u}_i$$\n",
    "\n",
    "we can see that\n",
    "\n",
    "$$\\mathbf{v}^T \\mathbf{H v} = \\big(\\sum_i c_i \\mathbf{u}_i \\big)^T H \\big(\\sum_i c_i \\mathbf{u}_i \\big)\n",
    "= \\sum_i \\lambda_i c_i^2 > 0$$\n",
    "\n",
    "in order for this expression to always hold, it needs that the eigenvalues are greater than 0, and when this happens the Hessian is going to be possitive definite\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.4 )\n",
    "Suppose we wish to use EM algorithm to maximize the posterior distribution over parameters $p(\\theta|X)$ for a model containing latent variables, where $X$ is observed data set. Show that E step remains the same as in the maximum likelihood case, whereas in the M step the quantity to be maximized is given by $$Q(\\theta,\\theta^{old})+\\ln p(\\theta)$$ where $$Q(\\theta,\\theta^{old})$$ is defined by \n",
    "\n",
    "$$Q(\\mathbf{\\theta, \\theta}^{old}) = \\sum_{\\mathbf{z}} p (\\mathbf{Z} | \\mathbf{X, \\theta}^{old}) \\ln p(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "Normally we maximize the $p(X|\\theta)$ but now we need to maximize $p(\\theta|X)$, we need to use bayes theorem for this\n",
    "\n",
    "$$p(\\theta|X) = \\frac{p(X|\\theta) p(\\theta)}{p(X)}$$\n",
    "\n",
    "Maximizing this is equal to maximize only the numerator and for convinience we maximize the logarithm of this distribution\n",
    "\n",
    "$$\\max_\\theta \\ln p(\\theta|X) = \\max_\\theta \\big( \\ln p(X|\\theta)  + \\ln  p(\\theta) \\big) $$\n",
    "\n",
    "note that the first term is the log likelihood of the normal EM step algorithm for this we need to maximize $ Q(\\theta, \\theta^{old})$ but because we have a new term $\\ln p(\\theta)$ now we need to maximize the following equation in the M step\n",
    "\n",
    "$$Q(\\theta,\\theta^{old})+\\ln p(\\theta)$$ \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.11 )\n",
    "In section 9.3.2 we obtained a relationship between $K$ means and EM for Gaussian Mixture by considering a mixture model in which all components have covariance $\\epsilon I$. Show that in the limit $\\epsilon \\rightarrow 0$, maximize the expected complete data log likelihood for his model, given by\n",
    "\n",
    "$$\\mathop{\\mathbb{E_z}} = \\lbrack \\ln p(\\mathbf{X,Z | \\mu, \\Sigma, \\pi}) \\rbrack = \\sum_{n=1}^N \\sum_{k=1}^K \\gamma(z_{nk}) \\{ \\ln \\pi_k + \\ln \\mathscr{N}( \\mathbf{x_n | \\mu_k, \\Sigma_k)}\\} $$\n",
    "\n",
    "is equivalent to minimizing the distortion measure $J$ for the K-means algorithm given by \n",
    "\n",
    "$$J = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} || \\mathbf{x}_n - \\mathbf{\\mu}_k ||^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "In the text it is shown that $\\delta(z_{nk}) \\rightarrow r_{nk}$ when $\\epsilon \\rightarrow 0$. Now we know that our optimization problem will not depend on $\\pi_k$ due the absence of latent variables. then our function to minimize will be \n",
    "\n",
    "$$E_z = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\ln \\mathscr{N}( \\mathbf{x_n} | \\mu_k, \\epsilon I)\\}$$\n",
    "\n",
    "and minimizing this is equal to minimize\n",
    "\n",
    "$$J = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} || \\mathbf{x}_n - \\mathbf{\\mu}_k ||^2 $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.15 ) Show that if we maximize the expected complete-data log likelihood function \n",
    "(9.55) \n",
    "\n",
    "$$ \\mathop{\\mathbb{E_z}} = \\lbrack \\ln p(\\mathbf{X,Z | \\mu, \\pi}) \\rbrack = \\sum_{n=1}^N \\sum_{k=1}^K \\gamma(z_{nk}) \\big\\{ \\ln \\pi_k + \\sum_{i=1}^D \\lbrack x_{ni} \\ln \\mu_{ki} + (1 - x_{ni}) \\ln (1 - \\mu_ {ki} ) \\rbrack   \\big\\} $$\n",
    "\n",
    "for a mixture of Bernoulli distribution with respect to $\\mu_k$, we obtain the M step equation\n",
    "\n",
    "$$\\pi_k = \\frac{N_k}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (HAYKIN):\n",
    "\n",
    "4.4 ) The momentum constant ()' is normally assigned a positive value in the range $0 \\leq \\alpha < 1$.\n",
    "Investigate the difference that would be made in the behavior of \n",
    "\n",
    "\n",
    "$$\\delta w_{ji} (n) = - \\eta \\sum_{t=0}^n \\alpha ^{n-t} \\frac{\\partial \\mathscr{E}(t)}{\\partial w_{ji}(t)}$$\n",
    "\n",
    "\n",
    "with respect to time t if $\\alpha$ was assigned a negative value in the range $-1 \\leq \\alpha < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "4.6 ) In Section 4.7 we presented qualitative arguments for the property of a multilayer per-ceptron classifier (using a logistic function for nonlinearity) that its outputs provide estimates of the a posteriori class probabilities. This property assumes that the size of the training set is large enough, and that the back-propagation algorithm used to train the network does not get stuck at a local minimum. Fill in the mathematical details of this property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "6.6 ) The inner-product kernel kernel $K(X_{i}, K_{j})$ is evaluated over a training sample $\\mathscr{T}$ of size N, yield-ing the N-by-N matrix: \n",
    "$$K = \\left \\{ K_{ij} \\right \\}_{i,j=1}^{N}$$\n",
    "\n",
    "where $K_{ij} = K(X_{i},X_{j})$. The matrix K is positive in that all of its elements have positive values. Using the similarity transformation: \n",
    "\n",
    "$$K = Q \\Delta Q^{T}$$\n",
    "\n",
    "where $\\Delta$ is a diagonal matrix of eigenvalues and Q is a matrix made up of the corresponding eigenvectors, formulate an expression for the inner-product kernel $K(X_{i}, X_{j})$ in terms of the eigenvalues and eigenvectors of matrix K. What conclusions can you draw from this representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "6.13 ) Compare the virtues and limitations of support vector machines with those of radial-basis function (RBF) networks with respect to the following task: \n",
    "\n",
    "* (1) Pattern classification \n",
    "* (2) Nonlinear regression . \n",
    "\n",
    "Do the same for support vector machines versus multilayer perceptron trained using the back propagation algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.15 ) The computer experiment described in section 6.6 was for the classification of two overlapping Gaussian Distributions. The following \"regularization\" parameter was used in that experiment: C = 0.1 The common width of the radial-basis functions used for con-structing the inner-product kernels was $\\sigma ^{2} = 4$. Repeat the computer experiment described in that section for the following two values of the regularization parameter:\n",
    "\n",
    "* C = 0.05\n",
    "* C = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the SVM using cv xopt for the quadratic optimization under linear constrains using the\n",
    "\n",
    "* (a) Using the voice data set, please perform the K - Cross Validation and report using ROC curves under the following kernels. \n",
    "\n",
    "    * 1. Polynomial Kernel $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_i + 1)^P $ with p defined before hand.\n",
    "    * 2. Radial Kernel $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp \\big\\{ - \\frac{1}{2 \\sigma^2} || \\mathbf{x}_i - \\mathbf{x}_j ||^2\\big\\}$ with $\\sigma^2$ defined before hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"voice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meanfreq    float64\n",
       "sd          float64\n",
       "median      float64\n",
       "Q25         float64\n",
       "Q75         float64\n",
       "IQR         float64\n",
       "skew        float64\n",
       "kurt        float64\n",
       "sp.ent      float64\n",
       "sfm         float64\n",
       "mode        float64\n",
       "centroid    float64\n",
       "meanfun     float64\n",
       "minfun      float64\n",
       "maxfun      float64\n",
       "meandom     float64\n",
       "mindom      float64\n",
       "maxdom      float64\n",
       "dfrange     float64\n",
       "modindx     float64\n",
       "label        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0L"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(data.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not have missing values and everything is type float, so we can proceed to create the labels vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new label column\n",
    "data[\"intlabel\"] = 0\n",
    "# Assign labels\n",
    "data.loc[data[\"label\"] == \"male\", \"intlabel\"] = 1\n",
    "data.loc[data[\"label\"] == \"female\", \"intlabel\"] = -1\n",
    "Y = data[\"intlabel\"].values\n",
    "X = data.drop([\"label\", \"intlabel\"], axis = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data to the unit norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "normalizer = StandardScaler()\n",
    "normalizer.fit(X)\n",
    "Xnorm = normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xnorm, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0466e+02 -5.0881e+01  2e+04  1e+02  1e-10\n",
      " 1: -9.1995e+00 -4.7803e+01  6e+02  4e+00  2e-10\n",
      " 2: -3.7548e+00 -3.8706e+01  1e+02  7e-01  2e-11\n",
      " 3: -2.3003e+00 -2.4123e+01  5e+01  2e-01  8e-12\n",
      " 4: -1.3989e+00 -1.1966e+01  2e+01  8e-02  3e-12\n",
      " 5: -1.0184e+00 -6.9308e+00  1e+01  4e-02  2e-12\n",
      " 6: -7.5178e-01 -3.8629e+00  5e+00  2e-02  7e-13\n",
      " 7: -6.6823e-01 -2.5418e+00  3e+00  8e-03  5e-13\n",
      " 8: -6.0767e-01 -1.8407e+00  2e+00  5e-03  3e-13\n",
      " 9: -5.7447e-01 -1.3910e+00  1e+00  2e-03  2e-13\n",
      "10: -5.5160e-01 -1.0973e+00  8e-01  1e-03  2e-13\n",
      "11: -5.6839e-01 -8.1546e-01  3e-01  5e-04  2e-13\n",
      "12: -5.8365e-01 -6.9798e-01  1e-01  2e-04  2e-13\n",
      "13: -5.9848e-01 -6.3102e-01  4e-02  4e-05  2e-13\n",
      "14: -6.0341e-01 -6.1114e-01  8e-03  1e-16  2e-13\n",
      "15: -6.0646e-01 -6.0745e-01  1e-03  1e-16  2e-13\n",
      "16: -6.0690e-01 -6.0693e-01  2e-05  1e-16  2e-13\n",
      "17: -6.0692e-01 -6.0692e-01  5e-07  1e-16  2e-13\n",
      "Optimal solution found.\n",
      "1\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.1354e+02 -4.6499e+02  2e+04  2e+01  1e-10\n",
      " 1: -2.5798e+01 -4.2231e+02  1e+03  1e+00  1e-10\n",
      " 2: -1.3888e+01 -2.7377e+02  5e+02  4e-01  3e-11\n",
      " 3: -7.0810e+00 -1.8540e+02  3e+02  2e-01  2e-11\n",
      " 4: -3.9816e+00 -7.8453e+01  1e+02  6e-02  6e-12\n",
      " 5: -2.1341e+00 -3.4304e+01  5e+01  2e-02  3e-12\n",
      " 6: -1.3977e+00 -1.6073e+01  2e+01  8e-03  2e-12\n",
      " 7: -1.0931e+00 -8.6077e+00  1e+01  4e-03  7e-13\n",
      " 8: -9.3006e-01 -4.9115e+00  6e+00  2e-03  5e-13\n",
      " 9: -8.1879e-01 -3.8254e+00  4e+00  9e-04  4e-13\n",
      "10: -7.7983e-01 -2.4564e+00  2e+00  4e-04  4e-13\n",
      "11: -8.4818e-01 -1.5619e+00  9e-01  1e-04  4e-13\n",
      "12: -9.2455e-01 -1.1377e+00  2e-01  1e-05  4e-13\n",
      "13: -9.6721e-01 -1.0236e+00  6e-02  2e-16  4e-13\n",
      "14: -9.8569e-01 -9.9753e-01  1e-02  4e-16  4e-13\n",
      "15: -9.9086e-01 -9.9117e-01  3e-04  2e-16  4e-13\n",
      "16: -9.9099e-01 -9.9100e-01  9e-06  2e-16  4e-13\n",
      "17: -9.9100e-01 -9.9100e-01  1e-07  5e-16  4e-13\n",
      "Optimal solution found.\n",
      "2\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.9844e+02 -4.7342e+03  3e+04  3e+00  2e-10\n",
      " 1: -1.2329e+02 -2.7146e+03  6e+03  5e-01  2e-10\n",
      " 2: -4.4615e+01 -1.5061e+03  3e+03  2e-01  1e-10\n",
      " 3: -1.3783e+01 -9.6123e+02  2e+03  6e-02  7e-11\n",
      " 4: -4.2138e+00 -3.6090e+02  5e+02  2e-02  2e-11\n",
      " 5: -4.0579e-01 -1.7261e+02  2e+02  6e-03  8e-12\n",
      " 6: -1.8446e-01 -6.6331e+01  9e+01  2e-03  4e-12\n",
      " 7: -2.1292e-01 -2.7593e+01  4e+01  6e-04  2e-12\n",
      " 8: -2.0742e-01 -1.4437e+01  2e+01  3e-04  7e-13\n",
      " 9: -2.4022e-01 -8.1652e+00  1e+01  1e-04  5e-13\n",
      "10: -3.0994e-01 -4.9849e+00  5e+00  4e-05  3e-13\n",
      "11: -5.9918e-01 -2.4930e+00  2e+00  1e-05  4e-13\n",
      "12: -6.8111e-01 -1.8049e+00  1e+00  3e-06  3e-13\n",
      "13: -8.6688e-01 -1.2639e+00  4e-01  7e-07  4e-13\n",
      "14: -9.3392e-01 -1.0884e+00  2e-01  7e-08  3e-13\n",
      "15: -9.7467e-01 -1.0156e+00  4e-02  5e-16  4e-13\n",
      "16: -9.8892e-01 -9.9632e-01  7e-03  5e-16  4e-13\n",
      "17: -9.9219e-01 -9.9236e-01  2e-04  5e-16  4e-13\n",
      "18: -9.9227e-01 -9.9227e-01  3e-06  4e-16  4e-13\n",
      "19: -9.9227e-01 -9.9227e-01  4e-08  4e-16  4e-13\n",
      "Optimal solution found.\n",
      "3\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.5141e+02 -1.4591e+05  4e+05  7e-01  1e-09\n",
      " 1: -6.1530e+01 -5.7193e+04  1e+05  2e-01  1e-09\n",
      " 2:  9.7008e+01 -2.2719e+04  4e+04  6e-02  6e-10\n",
      " 3:  9.8089e+01 -7.9384e+03  1e+04  2e-02  2e-10\n",
      " 4:  4.8730e+01 -2.1998e+03  4e+03  4e-03  8e-11\n",
      " 5:  2.6110e+01 -7.0857e+02  1e+03  9e-04  2e-11\n",
      " 6:  1.2252e+01 -2.2636e+02  3e+02  2e-04  7e-12\n",
      " 7:  6.2258e+00 -8.7966e+01  1e+02  6e-05  2e-12\n",
      " 8:  3.2571e+00 -3.9432e+01  5e+01  2e-05  1e-12\n",
      " 9:  1.5341e+00 -1.8528e+01  2e+01  9e-06  6e-13\n",
      "10:  2.2870e-01 -5.9978e+00  7e+00  2e-06  5e-13\n",
      "11: -2.4289e-01 -3.2865e+00  3e+00  3e-07  4e-13\n",
      "12: -5.6871e-01 -2.0529e+00  2e+00  1e-07  3e-13\n",
      "13: -7.6245e-01 -1.4950e+00  7e-01  1e-08  3e-13\n",
      "14: -8.9508e-01 -1.1785e+00  3e-01  3e-09  4e-13\n",
      "15: -9.5375e-01 -1.0481e+00  9e-02  5e-16  4e-13\n",
      "16: -9.7250e-01 -1.0180e+00  5e-02  2e-16  4e-13\n",
      "17: -9.8885e-01 -9.9660e-01  8e-03  2e-16  4e-13\n",
      "18: -9.9219e-01 -9.9237e-01  2e-04  2e-16  4e-13\n",
      "19: -9.9227e-01 -9.9227e-01  3e-06  2e-16  4e-13\n",
      "20: -9.9227e-01 -9.9227e-01  4e-08  6e-16  4e-13\n",
      "Optimal solution found.\n",
      "4\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  3.4419e+04 -1.1320e+07  3e+07  6e-01  1e-08\n",
      " 1:  4.3151e+04 -3.8389e+06  7e+06  1e-01  9e-09\n",
      " 2:  2.3133e+04 -1.5874e+06  3e+06  4e-02  4e-09\n",
      " 3:  1.0872e+04 -3.8611e+05  6e+05  7e-03  2e-09\n",
      " 4:  5.1310e+03 -1.3928e+05  2e+05  2e-03  5e-10\n",
      " 5:  2.4170e+03 -4.6951e+04  7e+04  5e-04  2e-10\n",
      " 6:  1.1786e+03 -1.5915e+04  2e+04  1e-04  5e-11\n",
      " 7:  6.7472e+02 -6.9504e+03  1e+04  4e-05  2e-11\n",
      " 8:  3.9808e+02 -2.9656e+03  4e+03  1e-05  7e-12\n",
      " 9:  1.6532e+02 -8.6306e+02  1e+03  3e-06  5e-12\n",
      "10:  3.6565e+01 -1.1176e+02  2e+02  3e-07  3e-12\n",
      "11:  4.7753e+00 -7.9332e+00  1e+01  2e-09  1e-12\n",
      "12:  1.3334e+00 -4.4325e+00  6e+00  6e-10  7e-13\n",
      "13:  1.6734e-01 -3.1061e+00  3e+00  2e-10  5e-13\n",
      "14: -4.2100e-01 -2.1773e+00  2e+00  3e-11  4e-13\n",
      "15: -6.3949e-01 -1.7975e+00  1e+00  7e-12  3e-13\n",
      "16: -8.4204e-01 -1.2801e+00  4e-01  1e-12  3e-13\n",
      "17: -9.0668e-01 -1.1338e+00  2e-01  1e-13  3e-13\n",
      "18: -9.5658e-01 -1.0467e+00  9e-02  2e-14  3e-13\n",
      "19: -9.7493e-01 -1.0145e+00  4e-02  9e-16  4e-13\n",
      "20: -9.9043e-01 -9.9453e-01  4e-03  5e-16  4e-13\n",
      "21: -9.9224e-01 -9.9230e-01  7e-05  5e-16  4e-13\n",
      "22: -9.9227e-01 -9.9227e-01  1e-06  4e-16  4e-13\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "import ML_Algorithms.Kernel as kern\n",
    "C_list = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "kernel = kern.Kernel._polykernel(3, 0)\n",
    "train_accs  = []\n",
    "test_accs = []\n",
    "predictors = []\n",
    "for i, C in enumerate(C_list):\n",
    "    clf = svm.SVMTrainer(kernel, C)\n",
    "    predictor = clf.train(X_train, y_train.astype(float))\n",
    "    predictors.append(predictor)\n",
    "    predicted_train = predictor.predict_all(X_train)\n",
    "    predicted_test = predictor.predict_all(X_test)\n",
    "    train_accs.append(metrics.accuracy(y_train, predicted_train))\n",
    "    test_accs.append(metrics.accuracy(y_test, predicted_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian/Radial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  4.7147e+03 -8.5005e+04  2e+05  3e-01  1e-14\n",
      " 1:  3.4001e+03 -1.2705e+04  2e+04  2e-02  7e-15\n",
      " 2:  8.0420e+02 -3.5227e+03  5e+03  3e-03  1e-14\n",
      " 3:  8.9621e+01 -1.4912e+03  2e+03  7e-04  5e-15\n",
      " 4: -1.5781e+02 -4.2258e+02  3e+02  2e-05  4e-15\n",
      " 5: -1.9260e+02 -2.6717e+02  7e+01  5e-06  2e-15\n",
      " 6: -2.0303e+02 -2.2605e+02  2e+01  1e-06  1e-15\n",
      " 7: -2.0676e+02 -2.1375e+02  7e+00  2e-07  1e-15\n",
      " 8: -2.0811e+02 -2.1000e+02  2e+00  3e-08  1e-15\n",
      " 9: -2.0856e+02 -2.0877e+02  2e-01  1e-09  1e-15\n",
      "10: -2.0862e+02 -2.0863e+02  7e-03  6e-12  1e-15\n",
      "11: -2.0863e+02 -2.0863e+02  2e-04  2e-13  1e-15\n",
      "12: -2.0863e+02 -2.0863e+02  5e-06  2e-14  1e-15\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "C2 = 10\n",
    "kernel2 = kern.Kernel.gaussian(1)\n",
    "clf2 = svm.SVMTrainer(kernel2, C2)\n",
    "predictor2 = clf2.train(X_train, y_train.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curves only evaluated for Polynomial kernel, Conclusion: the generation of ROC curves is not recommended for support vector machines because we need to change the bias term in each iteration to change the decission boundary, but then we need to reevaluate every sample iterating through the support vectors. The evaluation with the radial kernel was too computationally expensive. Therefore, it was decided to skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fbc2f303c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc2f3885d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor = predictors[4]\n",
    "rocs_train = predictor.roc(X_train, y_train)\n",
    "rocs_test = predictor.roc(X_test, y_test)\n",
    "\n",
    "plt.plot(rocs_train[:,0], rocs_train[:,1], color = 'r')\n",
    "plt.plot(rocs_test[:,0], rocs_test[:,1], color = 'b')\n",
    "plt.title(\"Train and test ROC, kernel: polynomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the multi-layer perceptron using sstochastic gradient descent and batch using the cross-entropy cost function and the ReLu activation function\n",
    "\n",
    "* (a) Using the voice data set, please perfeorm the K-Cross Validation to compare both algorithms. Report using ROC curves annd times of convergence in number of steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Homework 3 - Jose Vazquez-Espinoza\n",
    "\n",
    "Algorithm's implementation in: https://github.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:\n",
    "\n",
    "EM as a minorization algorithm (Hunter and Lange, 2004; Wu and Lange, 2007). A function $g(x,y)$ to said to minorize a function $f(x)$ if\n",
    "\n",
    "$$g(x,y) \\leq f(x), g(x,x) = f(x)$$\n",
    "\n",
    "for all $x, y$ in the domain. This is useful for maximizing $f(x)$ since is easy to show that $f(x)$ is non-decreasing under the update\n",
    "\n",
    "$$x^{s+1} = argmax_x g(x,x^s)$$\n",
    "\n",
    "There are analogous definitions for majorization, for minimizing a function $f(x)$. The resulting algorithms are known as MM algorithms, for “Minorize-Maximize” or “Majorize-Minimize.”\n",
    "\n",
    "Show that the EM algorithm (Section 8.5.2) is an example of an MM al-gorithm, using $Q(\\theta′,\\theta)+\\log P(Z|\\theta)−Q(\\theta, \\theta)$ to minorize the observed data log-likelihood $l(\\theta′; Z)$. (Note that only the first term involves the relevant\n",
    "parameter $\\theta′$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:\n",
    "\n",
    "### Problem (a):\n",
    "Consider a Gaussian mixture model in which the marginal distribution $p(z)$ for the latent variable is given by (9.10), and the conditional distribution $p(x|z)$ for the observed variable is given by (9.11). Show that the marginal distribution $p(x)$, obtained by summing $p(z)p(x|z)$ over all possible values of $z$, is a Gaussian\n",
    "mixture of the form (9.7).\n",
    "\n",
    "### Problem (b):\n",
    "Consider a mixture distribution of the form\n",
    "$$p(x) = \\sum_{k=1}^K \\pi_k p(\\mathbf{x}|k)$$\n",
    "where the elements of $x$ could be discrete or continuous or a combination of these.\n",
    "Denote the mean and covariance of $p(x|k)$ by $μ_k$ and $\\Sigma_k$ , respectively. Show that\n",
    "the mean and covariance of the mixture distribution are given by (9.49) and (9.50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "\n",
    "Implement using Python the EM algorithm for the mixture of Gaussians. After this:\n",
    "1. Create two classes composed of k Gaussians splitting the set in\n",
    "\n",
    "    1. 90% random training data\n",
    "    2. 10% random testing data\n",
    "    \n",
    "    \n",
    "2. Adjust the Gaussian Mixture to such classes.\n",
    "3. Use naive Bayesian classification to build the Confusion Matrix of your experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:\n",
    "\n",
    "Now, that you have the concept of k-Cross Validation and Confusion Matrix. Please using\n",
    "your previous algorithms\n",
    "\n",
    "1. Linear Regression under Gradient Descent\n",
    "2. Logistic Regression under Newton-Raphson\n",
    "\n",
    "and data sets, please divide the data sets in training, validation and testing data sets and\n",
    "provide the results for your previous experiments in terms of\n",
    "1. Confusion Matrix\n",
    "2. Accuracy and $F_1$ score\n",
    "3. ROC curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
